{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Database study\n",
    "### Project developer: \n",
    "Mar Balibrea Rull, marbr@kth.se\n",
    "\n",
    "### Instructions\n",
    "For each of the theoretical tasks 1a and 1c in the examination (see below), you are requested to answer them by means of simulations/tests using a Jupyter notebook. You may employ real datasets and learning algorithms, e.g., as implemented in Scikit-learn, or use synthetic classifiers/predictions/data, e.g., output by some random functions. You may use Numpy, pandas, Scikit-learn and SciPy (send me a request in case you would like to use any other package).\n",
    "\n",
    "You are expected to submit one notebook (by email to me), clearly separating the two tasks, with extensive comments explaining the assumptions and conclusions.\n",
    "\n",
    "The deadline for submission is March 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load general libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import metrics\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add complementary functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_normalization(df, avoid, normalizationtype = 'minmax'):\n",
    "    \n",
    "    _df = df.select_dtypes(include = ['float', 'int']).drop(columns = avoid)\n",
    "    \n",
    "    if normalizationtype is 'minmax':\n",
    "        \n",
    "        normalization = {col: (normalizationtype, _df[col].min(), _df[col].max()) for col in _df.columns}\n",
    "    elif normalizationtype is 'zscore':\n",
    "        \n",
    "        normalization = {col: (normalizationtype, _df[col].mean(), _df[col].std()) for col in _df.columns}\n",
    "    \n",
    "    df_out = apply_normalization(_df, normalization)\n",
    "    return df_out, normalization\n",
    "\n",
    "def apply_normalization(df, normalization):\n",
    "    \n",
    "    df_out = df.copy()\n",
    "    for col in normalization:\n",
    "        a = normalization[col][1]\n",
    "        b = normalization[col][2]\n",
    "    \n",
    "        if normalization[col][0] is 'minmax':\n",
    "            \n",
    "            df_out[col] = (df_out[col] - a)/(b - a) # using broadcasting\n",
    "            # df_out[col] = df_out[col].apply(lambda x: (x - a)/(b - a)) # using lambda\n",
    "            df_out[col].clip(0, 1, inplace = True)\n",
    "        elif normalization[col][0] is 'zscore':\n",
    "    \n",
    "            df_out[col] = df_out[col].apply(lambda x: (x - a)/b)\n",
    "    return df_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1a. Methodology\n",
    "Assume that we want to compare a new algorithm to a baseline\n",
    "algorithm, for some classification task. As we are not sure what\n",
    "hyper-parameter settings to use for the new algorithm, we will\n",
    "investigate 100 different settings for that, while we use a standard\n",
    "hyper-parameter setting for the baseline algorithm. We first randomly\n",
    "split a given dataset into two equal-sized halves; one for model\n",
    "building and one for testing. We then employ 10-fold cross-validation\n",
    "using the first half of the data, measuring the accuracy of each model\n",
    "generated from an algorithm and hyper-parameter setting. Assume that\n",
    "the best performing hyper-parameter setting for the new algorithm\n",
    "results in a higher (cross-validation) accuracy than the baseline\n",
    "algorithm. Should we expect to see the same relative performance,\n",
    "i.e., the new algorithm (with the best-performing hyper-parameter\n",
    "setting) outperforming the baseline (with the standard hyper-parameter\n",
    "setting), when the two models (trained on the entire first half) are\n",
    "evaluated on the second half of the data? Explain your reasoning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretation of the assignment**:\n",
    "\n",
    "I need:\n",
    "\n",
    "- A database\n",
    "- One algorithm with one hyper-parameter configuration\n",
    "- Another algorithm with 100 hyper-parameter configurations\n",
    "\n",
    "I will use the \"glass.txt\" database, as in past assignments. The partition in equal-sized halves is already done in \"glass_train.txt\" (for model building) and \"glass_test.txt\" (for testing). As for the algorithms, I will use MLPClassifier from sklearn as the baseline algorithm and RandomForestClassifier from sklearn as the new algorithm.\n",
    "\n",
    "Steps:\n",
    "\n",
    "1. Train and validate the baseline algorithm using 10-fold cross-validation on the first half of the dataset.\n",
    "2. Train and validate the new algorithm (100 configurations) using 10-fold cross-validation on the first half of the dataset.\n",
    "3. Make sure that the best performing configuration of the new algorithm outperforms the baseline algorithm in terms of accuracy.\n",
    "4. Evaluate the best performing configuration of the new algorithm and the baseline algorithm on the second database half."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from warnings import filterwarnings\n",
    "filterwarnings('ignore')\n",
    "\n",
    "# as both have the same length, we will use this partition:\n",
    "build = pd.read_csv('glass_train.txt')\n",
    "test = pd.read_csv('glass_test.txt')\n",
    "\n",
    "_, normalization = create_normalization(build, avoid = ['ID', 'CLASS'])\n",
    "build = apply_normalization(build, normalization)\n",
    "test = apply_normalization(test, normalization)\n",
    "\n",
    "kf = KFold(n_splits = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# FIRST STEP\n",
    "\n",
    "build_labels = build['CLASS']\n",
    "build_df = build.drop(columns = ['ID', 'CLASS'])\n",
    "\n",
    "# baseline algorithm\n",
    "\n",
    "accuracies = []\n",
    "for i_train, i_val in kf.split(build_df):\n",
    "\n",
    "    data_train = build_df.loc[i_train]\n",
    "    label_train = build_labels.loc[i_train]\n",
    "\n",
    "    data_val = build_df.loc[i_val]\n",
    "    label_val = build_labels.loc[i_val]\n",
    "\n",
    "    baseline = MLPClassifier()\n",
    "    baseline.fit(data_train, label_train)\n",
    "\n",
    "    p = baseline.predict(data_val)\n",
    "    accuracies.append(metrics.accuracy_score(label_val, p))\n",
    "\n",
    "baseline_acc = np.mean(accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SECOND STEP\n",
    "\n",
    "# new algorithm\n",
    "\n",
    "n_estimators_values = [10, 50, 100, 150, 200] # number of trees\n",
    "max_depth_values = [50, 70] # max depth trees\n",
    "min_samples_split_values = [4, 6, 8, 10, 12] # min samples to split\n",
    "class_weight_values = [None, 'balanced'] # class weight mode\n",
    "\n",
    "parameters = [(n_estimators, max_depth, min_samples, class_weight)\n",
    "              for n_estimators in n_estimators_values\n",
    "              for max_depth in max_depth_values\n",
    "              for min_samples in min_samples_split_values\n",
    "              for class_weight in class_weight_values]\n",
    "best_new_acc = 0\n",
    "best_acc_model = None\n",
    "models_better_baseline = 0\n",
    "\n",
    "for i in range(len(parameters)):\n",
    "\n",
    "    accuracies = []\n",
    "    for i_train, i_val in kf.split(build_df):\n",
    "\n",
    "        data_train = build_df.loc[i_train]\n",
    "        label_train = build_labels.loc[i_train]\n",
    "\n",
    "        data_val = build_df.loc[i_val]\n",
    "        label_val = build_labels.loc[i_val]\n",
    "\n",
    "        new = RandomForestClassifier(n_estimators = parameters[i][0],\n",
    "                                     max_depth = parameters[i][1],\n",
    "                                     min_samples_split = parameters[i][2],\n",
    "                                     class_weight = parameters[i][3],\n",
    "                                     random_state = 8)\n",
    "        new.fit(data_train, label_train)\n",
    "\n",
    "        p = new.predict(data_val)\n",
    "        accuracies.append(metrics.accuracy_score(label_val, p))\n",
    "\n",
    "    aux_new_acc = np.mean(accuracies)\n",
    "    \n",
    "    if aux_new_acc > best_new_acc:\n",
    "        \n",
    "        best_new_acc = aux_new_acc\n",
    "        best_new_acc_model = new\n",
    "        \n",
    "    models_better_baseline += aux_new_acc > baseline_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIRD STEP\n",
    "\n",
    "if baseline_acc >= best_new_acc:\n",
    "    \n",
    "    raise ValueError('Best new algorithm does not work better than the baseline')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model building accuracy for baseline algorithm: 0.516\n",
      "Model building accuracy for best new algorithm: 0.685\n",
      "Amount of new algorithms with higher accuracy: 100\n",
      "Test accuracy for baseline algorithm: 0.598\n",
      "Test accuracy for best new algorithm: 0.729\n"
     ]
    }
   ],
   "source": [
    "# FOURTH STEP\n",
    "\n",
    "test_labels = test['CLASS']\n",
    "test_df = test.drop(columns = ['ID', 'CLASS'])\n",
    "\n",
    "baseline_test_p = baseline.predict(test_df)\n",
    "baseline_test_acc = metrics.accuracy_score(test_labels, baseline_test_p)\n",
    "\n",
    "new_test_p = best_new_acc_model.predict(test_df)\n",
    "new_test_acc = metrics.accuracy_score(test_labels, new_test_p)\n",
    "\n",
    "print('Model building accuracy for baseline algorithm: ' + '{0:.3f}'.format(baseline_acc))\n",
    "print('Model building accuracy for best new algorithm: ' + '{0:.3f}'.format(best_new_acc))\n",
    "print('Amount of new algorithms with higher accuracy: ' + str(models_better_baseline))\n",
    "print('Test accuracy for baseline algorithm: ' + '{0:.3f}'.format(baseline_test_acc))\n",
    "print('Test accuracy for best new algorithm: ' + '{0:.3f}'.format(new_test_acc))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis\n",
    "\n",
    "We see that for these algorithms, the outperformance of the best new algorithm preserves in the testing, that is because the large majority (or all) of the configurations outperform the baseline algorithm during model building, so the probability is high. \n",
    "\n",
    "If we try *worse* configurations for the new algorithm so that less configurations perform better than the baseline, we would expect this doesn't happen due to the fact that the performance of the new algorithm is over-estimated.\n",
    "\n",
    "Let's try: the code below will be the exact same as second (we don't need to repeat the baseline) to fourth step from above, but with the parameters for the configurations of the new algorithm changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SECOND STEP (2nd experiment)\n",
    "\n",
    "# new algorithm\n",
    "\n",
    "n_estimators_values = [1, 3, 5, 7, 9] # number of trees\n",
    "max_depth_values = [20, 25] # max depth trees\n",
    "min_samples_split_values = [20, 25, 30, 35, 40] # min samples to split\n",
    "class_weight_values = [None, 'balanced'] # class weight mode\n",
    "\n",
    "parameters = [(n_estimators, max_depth, min_samples, class_weight)\n",
    "              for n_estimators in n_estimators_values\n",
    "              for max_depth in max_depth_values\n",
    "              for min_samples in min_samples_split_values\n",
    "              for class_weight in class_weight_values]\n",
    "best_new_acc_2 = 0\n",
    "best_acc_model_2 = None\n",
    "models_better_baseline_2 = 0\n",
    "\n",
    "for i in range(len(parameters)):\n",
    "\n",
    "    accuracies = []\n",
    "    for i_train, i_val in kf.split(build_df):\n",
    "\n",
    "        data_train = build_df.loc[i_train]\n",
    "        label_train = build_labels.loc[i_train]\n",
    "\n",
    "        data_val = build_df.loc[i_val]\n",
    "        label_val = build_labels.loc[i_val]\n",
    "\n",
    "        new = RandomForestClassifier(n_estimators = parameters[i][0],\n",
    "                                     max_depth = parameters[i][1],\n",
    "                                     min_samples_split = parameters[i][2],\n",
    "                                     class_weight = parameters[i][3],\n",
    "                                     random_state = 8)\n",
    "        new.fit(data_train, label_train)\n",
    "\n",
    "        p = new.predict(data_val)\n",
    "        accuracies.append(metrics.accuracy_score(label_val, p))\n",
    "\n",
    "    aux_new_acc = np.mean(accuracies)\n",
    "    \n",
    "    if aux_new_acc > best_new_acc_2:\n",
    "        \n",
    "        best_new_acc_2 = aux_new_acc\n",
    "        best_new_acc_model_2 = new\n",
    "        \n",
    "    models_better_baseline_2 += aux_new_acc > baseline_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model building accuracy for baseline algorithm: 0.516\n",
      "Model building accuracy for best new algorithm: 0.597 (was 0.685)\n",
      "Amount of new algorithms with higher accuracy: 38 (was 100)\n",
      "Test accuracy for baseline algorithm: 0.598\n",
      "Test accuracy for best new algorithm: 0.561 (was 0.729)\n"
     ]
    }
   ],
   "source": [
    "# THIRD STEP (2nd experiment)\n",
    "\n",
    "if baseline_acc >= best_new_acc_2:\n",
    "    \n",
    "    raise ValueError('Best new algorithm does not work better than the baseline')\n",
    "    \n",
    "    \n",
    "# FOURTH STEP (2nd experiment)\n",
    "\n",
    "# baseline done in 1st experiment\n",
    "\n",
    "new_test_p_2 = best_new_acc_model_2.predict(test_df)\n",
    "new_test_acc_2 = metrics.accuracy_score(test_labels, new_test_p_2)\n",
    "\n",
    "print('Model building accuracy for baseline algorithm: ' + '{0:.3f}'.format(baseline_acc))\n",
    "print('Model building accuracy for best new algorithm: ' + '{0:.3f}'.format(best_new_acc_2) + ' (was ' + '{0:.3f}'.format(best_new_acc) + ')')\n",
    "print('Amount of new algorithms with higher accuracy: ' + str(models_better_baseline_2) + ' (was ' + str(models_better_baseline) + ')')\n",
    "print('Test accuracy for baseline algorithm: ' + '{0:.3f}'.format(baseline_test_acc))\n",
    "print('Test accuracy for best new algorithm: ' + '{0:.3f}'.format(new_test_acc_2) + ' (was ' + '{0:.3f}'.format(new_test_acc) + ')')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis (part 2)\n",
    "\n",
    "As we can see, now the test accuracy for the best algorithm is lower even though the model building accuracy is still higher. We also see that the number of algorithms with higher accuracy is much lower. To try further, we can increase the minimum value of the `min_samples_split_values` parameter (which I have seen is the one that produces the more effect on the result)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1c. Performance metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assume that we have evaluated a binary classification model on a test\n",
    "set with 5000 instances; 4000 belonging to the majority class and 1000\n",
    "to the minority class. Assume that we have measured the accuracy and\n",
    "AUC, and also observed a much higher precision for the majority class\n",
    "than for the minority class. If we would evaluate the model on a\n",
    "class-balanced test set, which has been obtained from the first by\n",
    "keeping all instances from the minority class and sampling (without\n",
    "replacement) 1000 instances from the majority class, should we expect\n",
    "to see about the same accuracy and AUC as previously observed? Explain\n",
    "your reasoning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretation of the assignment**:\n",
    "\n",
    "I need:\n",
    "\n",
    "- A database\n",
    "- A model for binary classification\n",
    "\n",
    "As the databases we had for this course didn't have enough instances for the experiment, I looked for another one on the internet. I found [these](https://www.kaggle.com/hackerrank/developer-survey-2018) results from a developer survey made on 2018. However, it had too many features, so I decided to first remove some of them that were string or that had a lot of NULL values. I decided to use one of them as the class (if the developers were students or not).\n",
    "For the model, I will use the SCIKit MLPClassifier with standard parameters.\n",
    "\n",
    "Steps:\n",
    "\n",
    "1. First dataset creation: divide the dataset in two parts (training and testing), making sure that I have 5000 instances in the testing one (4000 belonging to the majority class). To make sure that the result is better for the majority class, I will build a training unbalanced dataset. For that purpose, I may not use the complete dataset.\n",
    "2. Train the model and test it by calculating accuracy and AUC.\n",
    "3. Second dataset creation: keep the previous division, but understample the testing one so that is stratified at 1000 instances per class.\n",
    "4. Test the model by calculating accuracy and AUC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from warnings import filterwarnings\n",
    "filterwarnings('ignore')\n",
    "\n",
    "# FIRST STEP\n",
    "\n",
    "df = pd.read_csv('developer_2.txt')\n",
    "# preprocessing: 'q1AgeBeginCoding' and 'q3Gender' have NULL values and have to be normalized\n",
    "df['q1AgeBeginCoding'].replace('#NULL!', np.nan, inplace = True)\n",
    "df['q3Gender'].replace('#NULL!', np.nan, inplace = True)\n",
    "df = df.astype('float')\n",
    "df = df.apply(lambda x: x.fillna(x.mean()), axis = 0)\n",
    "_, normalization = create_normalization(df, avoid = ['RespondentID', 'q8Student'])\n",
    "df = apply_normalization(df, normalization)\n",
    "\n",
    "# divide dataset in training and testing (MLPClassifier already takes validation from training)\n",
    "majorityclass = (len(df.loc[df['q8Student'] == 0]) < len(df.loc[df['q8Student'] == 1]))*1\n",
    "maj_df = df.loc[df['q8Student'] == majorityclass]\n",
    "min_df = df.loc[df['q8Student'] != majorityclass]\n",
    "\n",
    "maj_te = 4000; min_te = 1000; maj_tr = 10000; min_tr = 1000;\n",
    "if len(maj_df) < maj_te+maj_tr or len(min_df) < min_te+min_tr:\n",
    "    \n",
    "    raise ValueError('There are NOT enough instances for this distribution of classes')\n",
    "\n",
    "test_df = maj_df[:maj_te].append(min_df[:min_te]).sample(frac = 1, random_state = 2)\n",
    "train_df = maj_df[-maj_tr:].append(min_df[-min_tr:]).sample(frac = 1, random_state = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACC: 0.8008 ; AUC: 0.67156525\n"
     ]
    }
   ],
   "source": [
    "# SECOND STEP\n",
    "\n",
    "model = MLPClassifier()\n",
    "train_labels = train_df['q8Student']\n",
    "model.fit(train_df.drop(columns = ['RespondentID', 'q8Student']), train_labels)\n",
    "\n",
    "test_labels = test_df['q8Student']\n",
    "p = model.predict(test_df.drop(columns = ['RespondentID', 'q8Student']))\n",
    "pp = model.predict_proba(test_df.drop(columns = ['RespondentID', 'q8Student']))\n",
    "acc = metrics.accuracy_score(test_labels, p)\n",
    "auc = metrics.roc_auc_score(test_labels, pp[:, 1])\n",
    "print('ACC:', acc, '; AUC:', auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIRD STEP\n",
    "\n",
    "ttest_df = maj_df[:min_te].append(min_df[:min_te]).sample(frac = 1, random_state = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACC: 0.511 ; AUC: 0.6501625\n"
     ]
    }
   ],
   "source": [
    "# FORTH STEP\n",
    "\n",
    "ttest_labels = ttest_df['q8Student']\n",
    "p = model.predict(ttest_df.drop(columns = ['RespondentID', 'q8Student']))\n",
    "pp = model.predict_proba(ttest_df.drop(columns = ['RespondentID', 'q8Student']))\n",
    "acc = metrics.accuracy_score(ttest_labels, p)\n",
    "auc = metrics.roc_auc_score(ttest_labels, pp[:, 1])\n",
    "print('ACC:', acc, '; AUC:', auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis\n",
    "\n",
    "It is given that both accuracy and AUC are higher for the majority class. This is what happens in this experiment. To understand the results of both the accuracy and the AUC once the test set has been changed to 1000 instances per class, we have to know how the calculations for accuracy and AUC work. Accuracy basically counts how many instances have been labelled correctly, so when taking out instances of the majority class (which has a great percentage of being well classified), we lower the amount of correct instances, so the value of accuracy decreases. However, the AUC evaluates how good the model labels a positive instance ahead of a negative, and that generally changes through the training, not much the testing (apart from sample size reasons).\n",
    "\n",
    "### Comments\n",
    "\n",
    "The files that can be downloaded in the website linked above are:\n",
    "\n",
    "- Country-Code-Mapping.csv: mapping of countries to their country codes.\n",
    "- HackerRank-Developer-Survey-2018-Codebook.csv: mapping of each feature name to its question.\n",
    "- HackerRank-Developer-Survey-2018-Numeric-Mapping.csv: mapping of each feature name to all its possible numeric values with the explanation of them.\n",
    "- HackerRank-Developer-Survey-2018-Numeric.csv: data with numeric values.\n",
    "- HackerRank-Developer-Survey-2018-Values.csv: data with qualitative values."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
